{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # 함수 최적화\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "t_data: tensor([[ 3.],\n",
      "        [ 5.],\n",
      "        [ 7.],\n",
      "        [ 9.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 시드 설정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 입력값과 정답값 정의 (2차원 텐서로 정의)\n",
    "x_data = torch.FloatTensor([[1], [2], [3], [4], [5]])  # 입력값\n",
    "t_data = torch.FloatTensor([[3], [5], [7], [9], [11]])  # 정답값\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"x_data:\", x_data)\n",
    "print(\"t_data:\", t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=LinearRegressionModel()\n",
    "print(model)\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39.16685104370117 [Parameter containing:\n",
      "tensor([[0.9284]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3235], requires_grad=True)]\n",
      "100 0.08463062345981598 [Parameter containing:\n",
      "tensor([[2.1882]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3204], requires_grad=True)]\n",
      "200 0.04298952594399452 [Parameter containing:\n",
      "tensor([[2.1342]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5157], requires_grad=True)]\n",
      "300 0.021837208420038223 [Parameter containing:\n",
      "tensor([[2.0956]], requires_grad=True), Parameter containing:\n",
      "tensor([0.6548], requires_grad=True)]\n",
      "400 0.01109258271753788 [Parameter containing:\n",
      "tensor([[2.0681]], requires_grad=True), Parameter containing:\n",
      "tensor([0.7540], requires_grad=True)]\n",
      "500 0.00563468411564827 [Parameter containing:\n",
      "tensor([[2.0486]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8246], requires_grad=True)]\n",
      "600 0.0028622171375900507 [Parameter containing:\n",
      "tensor([[2.0346]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8750], requires_grad=True)]\n",
      "700 0.0014539161929860711 [Parameter containing:\n",
      "tensor([[2.0247]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9109], requires_grad=True)]\n",
      "800 0.0007385412463918328 [Parameter containing:\n",
      "tensor([[2.0176]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9365], requires_grad=True)]\n",
      "900 0.00037515218718908727 [Parameter containing:\n",
      "tensor([[2.0125]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9548], requires_grad=True)]\n",
      "1000 0.00019056595920119435 [Parameter containing:\n",
      "tensor([[2.0089]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9678], requires_grad=True)]\n",
      "1100 9.680409129941836e-05 [Parameter containing:\n",
      "tensor([[2.0064]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9770], requires_grad=True)]\n",
      "1200 4.91717473778408e-05 [Parameter containing:\n",
      "tensor([[2.0045]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9836], requires_grad=True)]\n",
      "1300 2.497919740562793e-05 [Parameter containing:\n",
      "tensor([[2.0032]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9883], requires_grad=True)]\n",
      "1400 1.2689983122982085e-05 [Parameter containing:\n",
      "tensor([[2.0023]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9917], requires_grad=True)]\n",
      "1500 6.447205578297144e-06 [Parameter containing:\n",
      "tensor([[2.0016]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9941], requires_grad=True)]\n",
      "1600 3.276181814726442e-06 [Parameter containing:\n",
      "tensor([[2.0012]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9958], requires_grad=True)]\n",
      "1700 1.6654261116855196e-06 [Parameter containing:\n",
      "tensor([[2.0008]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9970], requires_grad=True)]\n",
      "1800 8.460160643153358e-07 [Parameter containing:\n",
      "tensor([[2.0006]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9979], requires_grad=True)]\n",
      "1900 4.302897309571563e-07 [Parameter containing:\n",
      "tensor([[2.0004]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9985], requires_grad=True)]\n",
      "2000 2.1860606125301274e-07 [Parameter containing:\n",
      "tensor([[2.0003]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9989], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "epochs=2001 #전체 데이터 셋을 몇번 학습할 것인가\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y=model(x_data)\n",
    "    cost=F.mse_loss(y,t_data) #MSE 손실함수 계산 (평균 제곱 오차)\n",
    "\n",
    "    optimizer.zero_grad() # 안하면 기존 기울기에 더해지므로 0으로 초기화\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(epoch, cost.item(), list(model.parameters())) # 학습 횟수, 손실값, 기울기와 절편 값 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict =model(torch.FloatTensor([[7]])) # 2차원형태로 넣어줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) (25,)\n",
      "(25, 3) (25, 1)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_csv('../data/data-01-test-score.csv', header=None) # 헤더가 없는 데이터\n",
    "x_data=df.values[:,:-1] #0부터 마지막 전까지\n",
    "x_data\n",
    "t_data=df.values[:,-1] #마지막 열만\n",
    "print(x_data.shape, t_data.shape)\n",
    "t_data=t_data.reshape(-1, 1) #-1은 행을 자동으로 계산하라는 의미\n",
    "print(x_data.shape, t_data.shape)\n",
    "print(type(x_data), type(t_data)) # 훈련시키기 위해 텐서로 변환해야함\n",
    "\n",
    "x_data1=torch.from_numpy(x_data).float()\n",
    "t_data1=torch.from_numpy(t_data).float()\n",
    "print(type(x_data1), type(t_data1))\n",
    "\n",
    "indim=x_data1.size(1) # 0이면 2차원의 값, 1이면 1차원의 값\n",
    "print(indim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(indim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultiLinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20001], Loss: 20483.0645\n",
      "Weight: [[-0.09591647  0.31669506  0.10380485]], Bias: [-0.07040991]\n",
      "Epoch [100/20001], Loss: 20.2364\n",
      "Weight: [[0.45392194 0.8682395  0.67179894]], Bias: [-0.06359691]\n",
      "Epoch [200/20001], Loss: 13.2856\n",
      "Weight: [[0.46369904 0.8767572  0.6840848 ]], Bias: [-0.06347523]\n",
      "Epoch [300/20001], Loss: 13.2201\n",
      "Weight: [[0.46356934 0.8753202  0.68616766]], Bias: [-0.06347632]\n",
      "Epoch [400/20001], Loss: 13.1575\n",
      "Weight: [[0.4632589 0.8737076 0.6880556]], Bias: [-0.06347996]\n",
      "Epoch [500/20001], Loss: 13.0955\n",
      "Weight: [[0.46294597 0.87209976 0.6899308 ]], Bias: [-0.06348369]\n",
      "Epoch [600/20001], Loss: 13.0340\n",
      "Weight: [[0.46263352 0.87049925 0.69179875]], Bias: [-0.06348741]\n",
      "Epoch [700/20001], Loss: 12.9730\n",
      "Weight: [[0.4623229  0.8689058  0.69365776]], Bias: [-0.06349114]\n",
      "Epoch [800/20001], Loss: 12.9126\n",
      "Weight: [[0.46201295 0.8673196  0.6955086 ]], Bias: [-0.06349486]\n",
      "Epoch [900/20001], Loss: 12.8528\n",
      "Weight: [[0.46170393 0.8657412  0.6973522 ]], Bias: [-0.06349859]\n",
      "Epoch [1000/20001], Loss: 12.7934\n",
      "Weight: [[0.46139652 0.8641694  0.699187  ]], Bias: [-0.06350231]\n",
      "Epoch [1100/20001], Loss: 12.7346\n",
      "Weight: [[0.46108955 0.8626044  0.7010143 ]], Bias: [-0.06350604]\n",
      "Epoch [1200/20001], Loss: 12.6763\n",
      "Weight: [[0.46078297 0.86104673 0.7028344 ]], Bias: [-0.06350976]\n",
      "Epoch [1300/20001], Loss: 12.6185\n",
      "Weight: [[0.46047816 0.859496   0.7046459 ]], Bias: [-0.06351349]\n",
      "Epoch [1400/20001], Loss: 12.5612\n",
      "Weight: [[0.46017417 0.8579534  0.706449  ]], Bias: [-0.06351721]\n",
      "Epoch [1500/20001], Loss: 12.5044\n",
      "Weight: [[0.45987055 0.8564179  0.7082449 ]], Bias: [-0.06352102]\n",
      "Epoch [1600/20001], Loss: 12.4481\n",
      "Weight: [[0.45956844 0.85488904 0.71003264]], Bias: [-0.06352549]\n",
      "Epoch [1700/20001], Loss: 12.3924\n",
      "Weight: [[0.45926744 0.85336685 0.71181244]], Bias: [-0.06352996]\n",
      "Epoch [1800/20001], Loss: 12.3371\n",
      "Weight: [[0.4589668  0.8518516  0.71358514]], Bias: [-0.06353443]\n",
      "Epoch [1900/20001], Loss: 12.2822\n",
      "Weight: [[0.4586679  0.85034287 0.71535045]], Bias: [-0.0635389]\n",
      "Epoch [2000/20001], Loss: 12.2279\n",
      "Weight: [[0.45836988 0.8488416  0.71710694]], Bias: [-0.06354337]\n",
      "Epoch [2100/20001], Loss: 12.1741\n",
      "Weight: [[0.45807186 0.8473475  0.7188566 ]], Bias: [-0.06354784]\n",
      "Epoch [2200/20001], Loss: 12.1207\n",
      "Weight: [[0.45777485 0.8458602  0.72059923]], Bias: [-0.06355231]\n",
      "Epoch [2300/20001], Loss: 12.0678\n",
      "Weight: [[0.45747954 0.84437954 0.72233313]], Bias: [-0.06355678]\n",
      "Epoch [2400/20001], Loss: 12.0154\n",
      "Weight: [[0.4571845  0.84290564 0.7240595 ]], Bias: [-0.06356125]\n",
      "Epoch [2500/20001], Loss: 11.9634\n",
      "Weight: [[0.4568909  0.8414384  0.72577894]], Bias: [-0.06356572]\n",
      "Epoch [2600/20001], Loss: 11.9119\n",
      "Weight: [[0.45659852 0.8399774  0.7274912 ]], Bias: [-0.06357019]\n",
      "Epoch [2700/20001], Loss: 11.8608\n",
      "Weight: [[0.45630646 0.8385231  0.72919613]], Bias: [-0.06357466]\n",
      "Epoch [2800/20001], Loss: 11.8102\n",
      "Weight: [[0.45601484 0.83707595 0.7308937 ]], Bias: [-0.06357913]\n",
      "Epoch [2900/20001], Loss: 11.7600\n",
      "Weight: [[0.45572495 0.8356354  0.7325836 ]], Bias: [-0.0635836]\n",
      "Epoch [3000/20001], Loss: 11.7103\n",
      "Weight: [[0.45543578 0.83420163 0.73426586]], Bias: [-0.06358851]\n",
      "Epoch [3100/20001], Loss: 11.6610\n",
      "Weight: [[0.4551467  0.83277464 0.7359411 ]], Bias: [-0.06359372]\n",
      "Epoch [3200/20001], Loss: 11.6122\n",
      "Weight: [[0.45485932 0.83135396 0.73760855]], Bias: [-0.06359894]\n",
      "Epoch [3300/20001], Loss: 11.5638\n",
      "Weight: [[0.4545732 0.8299396 0.7392688]], Bias: [-0.06360415]\n",
      "Epoch [3400/20001], Loss: 11.5158\n",
      "Weight: [[0.45428714 0.82853156 0.74092233]], Bias: [-0.06360937]\n",
      "Epoch [3500/20001], Loss: 11.4682\n",
      "Weight: [[0.45400262 0.82713    0.74256915]], Bias: [-0.06361458]\n",
      "Epoch [3600/20001], Loss: 11.4210\n",
      "Weight: [[0.45371947 0.8257343  0.7442082 ]], Bias: [-0.0636198]\n",
      "Epoch [3700/20001], Loss: 11.3743\n",
      "Weight: [[0.45343634 0.82434565 0.74584   ]], Bias: [-0.06362502]\n",
      "Epoch [3800/20001], Loss: 11.3279\n",
      "Weight: [[0.45315388 0.8229637  0.7474653 ]], Bias: [-0.06363023]\n",
      "Epoch [3900/20001], Loss: 11.2820\n",
      "Weight: [[0.45287234 0.8215881  0.7490835 ]], Bias: [-0.06363545]\n",
      "Epoch [4000/20001], Loss: 11.2365\n",
      "Weight: [[0.452592   0.82021874 0.7506943 ]], Bias: [-0.06364066]\n",
      "Epoch [4100/20001], Loss: 11.1914\n",
      "Weight: [[0.45231217 0.81885576 0.75229836]], Bias: [-0.06364588]\n",
      "Epoch [4200/20001], Loss: 11.1467\n",
      "Weight: [[0.45203382 0.81749845 0.7538951 ]], Bias: [-0.06365109]\n",
      "Epoch [4300/20001], Loss: 11.1024\n",
      "Weight: [[0.45175666 0.81614697 0.7554852 ]], Bias: [-0.06365631]\n",
      "Epoch [4400/20001], Loss: 11.0584\n",
      "Weight: [[0.4514795  0.8148019  0.75706875]], Bias: [-0.06366152]\n",
      "Epoch [4500/20001], Loss: 11.0149\n",
      "Weight: [[0.4512029 0.8134632 0.7586459]], Bias: [-0.06366674]\n",
      "Epoch [4600/20001], Loss: 10.9717\n",
      "Weight: [[0.45092803 0.8121308  0.76021576]], Bias: [-0.06367268]\n",
      "Epoch [4700/20001], Loss: 10.9289\n",
      "Weight: [[0.45065358 0.81080467 0.76177835]], Bias: [-0.06367864]\n",
      "Epoch [4800/20001], Loss: 10.8865\n",
      "Weight: [[0.45037943 0.80948496 0.76333433]], Bias: [-0.0636846]\n",
      "Epoch [4900/20001], Loss: 10.8445\n",
      "Weight: [[0.45010677 0.80817103 0.76488316]], Bias: [-0.06369056]\n",
      "Epoch [5000/20001], Loss: 10.8028\n",
      "Weight: [[0.44983557 0.80686295 0.7664252 ]], Bias: [-0.06369652]\n",
      "Epoch [5100/20001], Loss: 10.7615\n",
      "Weight: [[0.44956437 0.8055608  0.7679609 ]], Bias: [-0.06370248]\n",
      "Epoch [5200/20001], Loss: 10.7206\n",
      "Weight: [[0.44929412 0.8042646  0.76949036]], Bias: [-0.06370844]\n",
      "Epoch [5300/20001], Loss: 10.6800\n",
      "Weight: [[0.44902563 0.80297387 0.7710132 ]], Bias: [-0.0637144]\n",
      "Epoch [5400/20001], Loss: 10.6398\n",
      "Weight: [[0.44875738 0.8016894  0.77252895]], Bias: [-0.06372036]\n",
      "Epoch [5500/20001], Loss: 10.5999\n",
      "Weight: [[0.44848916 0.8004111  0.77403855]], Bias: [-0.06372632]\n",
      "Epoch [5600/20001], Loss: 10.5604\n",
      "Weight: [[0.4482219 0.7991388 0.775542 ]], Bias: [-0.06373228]\n",
      "Epoch [5700/20001], Loss: 10.5213\n",
      "Weight: [[0.44795617 0.7978723  0.7770383 ]], Bias: [-0.06373824]\n",
      "Epoch [5800/20001], Loss: 10.4825\n",
      "Weight: [[0.44769093 0.7966117  0.77852774]], Bias: [-0.0637442]\n",
      "Epoch [5900/20001], Loss: 10.4440\n",
      "Weight: [[0.44742632 0.79535675 0.78001106]], Bias: [-0.06375016]\n",
      "Epoch [6000/20001], Loss: 10.4058\n",
      "Weight: [[0.44716376 0.7941067  0.78148806]], Bias: [-0.06375612]\n",
      "Epoch [6100/20001], Loss: 10.3680\n",
      "Weight: [[0.4469015  0.7928623  0.78295887]], Bias: [-0.06376208]\n",
      "Epoch [6200/20001], Loss: 10.3305\n",
      "Weight: [[0.44663924 0.7916239  0.7844237 ]], Bias: [-0.06376823]\n",
      "Epoch [6300/20001], Loss: 10.2934\n",
      "Weight: [[0.44637793 0.7903914  0.7858824 ]], Bias: [-0.06377494]\n",
      "Epoch [6400/20001], Loss: 10.2566\n",
      "Weight: [[0.44611758 0.7891647  0.78733414]], Bias: [-0.06378164]\n",
      "Epoch [6500/20001], Loss: 10.2201\n",
      "Weight: [[0.4458583  0.7879439  0.78877896]], Bias: [-0.06378835]\n",
      "Epoch [6600/20001], Loss: 10.1839\n",
      "Weight: [[0.44559962 0.78672874 0.7902179 ]], Bias: [-0.06379505]\n",
      "Epoch [6700/20001], Loss: 10.1481\n",
      "Weight: [[0.4453417  0.78551894 0.7916507 ]], Bias: [-0.06380176]\n",
      "Epoch [6800/20001], Loss: 10.1125\n",
      "Weight: [[0.44508484 0.7843148  0.79307723]], Bias: [-0.06380846]\n",
      "Epoch [6900/20001], Loss: 10.0773\n",
      "Weight: [[0.44482854 0.78311616 0.79449767]], Bias: [-0.06381517]\n",
      "Epoch [7000/20001], Loss: 10.0424\n",
      "Weight: [[0.4445727  0.78192323 0.79591227]], Bias: [-0.06382187]\n",
      "Epoch [7100/20001], Loss: 10.0078\n",
      "Weight: [[0.4443184  0.78073514 0.7973211 ]], Bias: [-0.06382858]\n",
      "Epoch [7200/20001], Loss: 9.9735\n",
      "Weight: [[0.44406492 0.7795526  0.7987233 ]], Bias: [-0.06383529]\n",
      "Epoch [7300/20001], Loss: 9.9395\n",
      "Weight: [[0.4438116  0.7783758  0.80011916]], Bias: [-0.06384199]\n",
      "Epoch [7400/20001], Loss: 9.9058\n",
      "Weight: [[0.44355863 0.77720463 0.80150944]], Bias: [-0.0638487]\n",
      "Epoch [7500/20001], Loss: 9.8724\n",
      "Weight: [[0.443307   0.77603877 0.8028938 ]], Bias: [-0.0638554]\n",
      "Epoch [7600/20001], Loss: 9.8393\n",
      "Weight: [[0.4430561  0.77487826 0.8042717 ]], Bias: [-0.06386211]\n",
      "Epoch [7700/20001], Loss: 9.8065\n",
      "Weight: [[0.44280586 0.7737231  0.8056433 ]], Bias: [-0.06386881]\n",
      "Epoch [7800/20001], Loss: 9.7739\n",
      "Weight: [[0.44255686 0.77257276 0.8070094 ]], Bias: [-0.06387552]\n",
      "Epoch [7900/20001], Loss: 9.7417\n",
      "Weight: [[0.44230884 0.77142787 0.80836976]], Bias: [-0.06388222]\n",
      "Epoch [8000/20001], Loss: 9.7097\n",
      "Weight: [[0.44206148 0.7702877  0.8097244 ]], Bias: [-0.06388918]\n",
      "Epoch [8100/20001], Loss: 9.6780\n",
      "Weight: [[0.44181412 0.76915306 0.81107336]], Bias: [-0.06389663]\n",
      "Epoch [8200/20001], Loss: 9.6466\n",
      "Weight: [[0.44156724 0.7680239  0.8124169 ]], Bias: [-0.06390408]\n",
      "Epoch [8300/20001], Loss: 9.6155\n",
      "Weight: [[0.441322   0.76690006 0.81375414]], Bias: [-0.06391153]\n",
      "Epoch [8400/20001], Loss: 9.5847\n",
      "Weight: [[0.44107708 0.7657815  0.8150849 ]], Bias: [-0.06391899]\n",
      "Epoch [8500/20001], Loss: 9.5541\n",
      "Weight: [[0.44083306 0.7646681  0.81640977]], Bias: [-0.06392644]\n",
      "Epoch [8600/20001], Loss: 9.5238\n",
      "Weight: [[0.4405899  0.76355946 0.81772923]], Bias: [-0.06393389]\n",
      "Epoch [8700/20001], Loss: 9.4937\n",
      "Weight: [[0.44034758 0.7624565  0.81904304]], Bias: [-0.06394134]\n",
      "Epoch [8800/20001], Loss: 9.4639\n",
      "Weight: [[0.44010618 0.76135814 0.8203512 ]], Bias: [-0.06394879]\n",
      "Epoch [8900/20001], Loss: 9.4344\n",
      "Weight: [[0.43986478 0.7602648  0.8216541 ]], Bias: [-0.06395624]\n",
      "Epoch [9000/20001], Loss: 9.4052\n",
      "Weight: [[0.43962464 0.7591761  0.82295173]], Bias: [-0.06396369]\n",
      "Epoch [9100/20001], Loss: 9.3761\n",
      "Weight: [[0.43938553 0.7580923  0.8242433 ]], Bias: [-0.06397114]\n",
      "Epoch [9200/20001], Loss: 9.3474\n",
      "Weight: [[0.4391471  0.7570136  0.82552874]], Bias: [-0.06397859]\n",
      "Epoch [9300/20001], Loss: 9.3189\n",
      "Weight: [[0.4389087  0.75594074 0.8268091 ]], Bias: [-0.06398604]\n",
      "Epoch [9400/20001], Loss: 9.2907\n",
      "Weight: [[0.4386706  0.75487304 0.8280843 ]], Bias: [-0.06399349]\n",
      "Epoch [9500/20001], Loss: 9.2627\n",
      "Weight: [[0.43843403 0.75381    0.8293535 ]], Bias: [-0.06400094]\n",
      "Epoch [9600/20001], Loss: 9.2349\n",
      "Weight: [[0.4381986  0.75275165 0.830617  ]], Bias: [-0.06400839]\n",
      "Epoch [9700/20001], Loss: 9.2074\n",
      "Weight: [[0.4379632  0.75169814 0.83187467]], Bias: [-0.06401584]\n",
      "Epoch [9800/20001], Loss: 9.1802\n",
      "Weight: [[0.4377291  0.7506491  0.83312714]], Bias: [-0.06402329]\n",
      "Epoch [9900/20001], Loss: 9.1532\n",
      "Weight: [[0.437496   0.7496049  0.83437467]], Bias: [-0.06403074]\n",
      "Epoch [10000/20001], Loss: 9.1264\n",
      "Weight: [[0.43726355 0.748565   0.83561677]], Bias: [-0.06403887]\n",
      "Epoch [10100/20001], Loss: 9.0998\n",
      "Weight: [[0.4370311  0.74753004 0.8368538 ]], Bias: [-0.06404707]\n",
      "Epoch [10200/20001], Loss: 9.0735\n",
      "Weight: [[0.436799   0.7464999  0.83808607]], Bias: [-0.06405526]\n",
      "Epoch [10300/20001], Loss: 9.0474\n",
      "Weight: [[0.4365676 0.7454747 0.8393126]], Bias: [-0.06406346]\n",
      "Epoch [10400/20001], Loss: 9.0215\n",
      "Weight: [[0.43633744 0.74445516 0.84053314]], Bias: [-0.06407166]\n",
      "Epoch [10500/20001], Loss: 8.9959\n",
      "Weight: [[0.43610796 0.7434404  0.84174836]], Bias: [-0.06407985]\n",
      "Epoch [10600/20001], Loss: 8.9705\n",
      "Weight: [[0.4358792  0.7424298  0.84295833]], Bias: [-0.06408805]\n",
      "Epoch [10700/20001], Loss: 8.9453\n",
      "Weight: [[0.43565175 0.74142295 0.8441628 ]], Bias: [-0.06409624]\n",
      "Epoch [10800/20001], Loss: 8.9204\n",
      "Weight: [[0.43542463 0.7404214  0.8453623 ]], Bias: [-0.06410444]\n",
      "Epoch [10900/20001], Loss: 8.8956\n",
      "Weight: [[0.43519813 0.7394243  0.846557  ]], Bias: [-0.06411263]\n",
      "Epoch [11000/20001], Loss: 8.8711\n",
      "Weight: [[0.43497163 0.7384317  0.8477471 ]], Bias: [-0.06412083]\n",
      "Epoch [11100/20001], Loss: 8.8468\n",
      "Weight: [[0.4347465  0.7374429  0.84893245]], Bias: [-0.06412902]\n",
      "Epoch [11200/20001], Loss: 8.8227\n",
      "Weight: [[0.43452182 0.73645943 0.850112  ]], Bias: [-0.06413722]\n",
      "Epoch [11300/20001], Loss: 8.7988\n",
      "Weight: [[0.4342983  0.73548114 0.851286  ]], Bias: [-0.06414542]\n",
      "Epoch [11400/20001], Loss: 8.7751\n",
      "Weight: [[0.4340748 0.7345074 0.8524545]], Bias: [-0.06415361]\n",
      "Epoch [11500/20001], Loss: 8.7517\n",
      "Weight: [[0.43385226 0.7335376  0.8536182 ]], Bias: [-0.06416181]\n",
      "Epoch [11600/20001], Loss: 8.7284\n",
      "Weight: [[0.4336301 0.732572  0.8547771]], Bias: [-0.06417]\n",
      "Epoch [11700/20001], Loss: 8.7054\n",
      "Weight: [[0.43340904 0.73161197 0.85593075]], Bias: [-0.0641782]\n",
      "Epoch [11800/20001], Loss: 8.6825\n",
      "Weight: [[0.4331885 0.7306559 0.8570798]], Bias: [-0.06418639]\n",
      "Epoch [11900/20001], Loss: 8.6599\n",
      "Weight: [[0.43296883 0.72970337 0.8582242 ]], Bias: [-0.06419459]\n",
      "Epoch [12000/20001], Loss: 8.6374\n",
      "Weight: [[0.43275005 0.72875565 0.85936296]], Bias: [-0.06420279]\n",
      "Epoch [12100/20001], Loss: 8.6152\n",
      "Weight: [[0.4325324  0.72781193 0.86049724]], Bias: [-0.06421137]\n",
      "Epoch [12200/20001], Loss: 8.5931\n",
      "Weight: [[0.4323148  0.7268724  0.86162645]], Bias: [-0.06422031]\n",
      "Epoch [12300/20001], Loss: 8.5712\n",
      "Weight: [[0.43209726 0.725937   0.8627516 ]], Bias: [-0.06422925]\n",
      "Epoch [12400/20001], Loss: 8.5495\n",
      "Weight: [[0.4318797 0.7250072 0.8638722]], Bias: [-0.06423819]\n",
      "Epoch [12500/20001], Loss: 8.5281\n",
      "Weight: [[0.43166354 0.72408175 0.8649871 ]], Bias: [-0.06424713]\n",
      "Epoch [12600/20001], Loss: 8.5068\n",
      "Weight: [[0.43144852 0.72316027 0.8660967 ]], Bias: [-0.06425607]\n",
      "Epoch [12700/20001], Loss: 8.4857\n",
      "Weight: [[0.43123388 0.72224265 0.86720085]], Bias: [-0.06426501]\n",
      "Epoch [12800/20001], Loss: 8.4648\n",
      "Weight: [[0.43101943 0.7213307  0.8683009 ]], Bias: [-0.06427395]\n",
      "Epoch [12900/20001], Loss: 8.4440\n",
      "Weight: [[0.43080673 0.72042143 0.86939687]], Bias: [-0.06428289]\n",
      "Epoch [13000/20001], Loss: 8.4235\n",
      "Weight: [[0.43059507 0.71951544 0.87048763]], Bias: [-0.06429183]\n",
      "Epoch [13100/20001], Loss: 8.4031\n",
      "Weight: [[0.43038347 0.7186149  0.8715736 ]], Bias: [-0.06430078]\n",
      "Epoch [13200/20001], Loss: 8.3829\n",
      "Weight: [[0.43017188 0.71771795 0.87265575]], Bias: [-0.06430972]\n",
      "Epoch [13300/20001], Loss: 8.3629\n",
      "Weight: [[0.42996067 0.7168246  0.87373406]], Bias: [-0.06431866]\n",
      "Epoch [13400/20001], Loss: 8.3430\n",
      "Weight: [[0.42975032 0.7159365  0.87480694]], Bias: [-0.0643276]\n",
      "Epoch [13500/20001], Loss: 8.3233\n",
      "Weight: [[0.42954135 0.71505266 0.8758743 ]], Bias: [-0.06433654]\n",
      "Epoch [13600/20001], Loss: 8.3038\n",
      "Weight: [[0.4293324 0.7141726 0.8769367]], Bias: [-0.06434548]\n",
      "Epoch [13700/20001], Loss: 8.2845\n",
      "Weight: [[0.42912403 0.7132964  0.87799484]], Bias: [-0.06435442]\n",
      "Epoch [13800/20001], Loss: 8.2654\n",
      "Weight: [[0.428916   0.7124255  0.87904906]], Bias: [-0.06436336]\n",
      "Epoch [13900/20001], Loss: 8.2464\n",
      "Weight: [[0.42871007 0.71155715 0.88009816]], Bias: [-0.0643723]\n",
      "Epoch [14000/20001], Loss: 8.2276\n",
      "Weight: [[0.42850378 0.7106929  0.8811422 ]], Bias: [-0.06438124]\n",
      "Epoch [14100/20001], Loss: 8.2089\n",
      "Weight: [[0.42829815 0.7098335  0.8821824 ]], Bias: [-0.06439018]\n",
      "Epoch [14200/20001], Loss: 8.1904\n",
      "Weight: [[0.42809305 0.7089771  0.8832191 ]], Bias: [-0.06439912]\n",
      "Epoch [14300/20001], Loss: 8.1721\n",
      "Weight: [[0.427889   0.70812476 0.8842503 ]], Bias: [-0.06440806]\n",
      "Epoch [14400/20001], Loss: 8.1539\n",
      "Weight: [[0.4276859 0.7072763 0.8852774]], Bias: [-0.06441712]\n",
      "Epoch [14500/20001], Loss: 8.1359\n",
      "Weight: [[0.42748293 0.7064312  0.8863    ]], Bias: [-0.0644268]\n",
      "Epoch [14600/20001], Loss: 8.1181\n",
      "Weight: [[0.42728028 0.7055908  0.88731855]], Bias: [-0.06443649]\n",
      "Epoch [14700/20001], Loss: 8.1004\n",
      "Weight: [[0.42707762 0.704755   0.8883323 ]], Bias: [-0.06444617]\n",
      "Epoch [14800/20001], Loss: 8.0828\n",
      "Weight: [[0.42687646 0.70392215 0.88934183]], Bias: [-0.06445586]\n",
      "Epoch [14900/20001], Loss: 8.0654\n",
      "Weight: [[0.42667478 0.70309365 0.890347  ]], Bias: [-0.06446555]\n",
      "Epoch [15000/20001], Loss: 8.0482\n",
      "Weight: [[0.42647502 0.7022696  0.8913476 ]], Bias: [-0.06447523]\n",
      "Epoch [15100/20001], Loss: 8.0311\n",
      "Weight: [[0.42627567 0.7014485  0.892343  ]], Bias: [-0.06448492]\n",
      "Epoch [15200/20001], Loss: 8.0142\n",
      "Weight: [[0.42607668 0.7006318  0.8933344 ]], Bias: [-0.0644946]\n",
      "Epoch [15300/20001], Loss: 7.9974\n",
      "Weight: [[0.425879   0.69981766 0.8943227 ]], Bias: [-0.06450429]\n",
      "Epoch [15400/20001], Loss: 7.9808\n",
      "Weight: [[0.4256823  0.69900703 0.89530617]], Bias: [-0.06451397]\n",
      "Epoch [15500/20001], Loss: 7.9643\n",
      "Weight: [[0.4254856 0.6982006 0.8962855]], Bias: [-0.06452366]\n",
      "Epoch [15600/20001], Loss: 7.9479\n",
      "Weight: [[0.42528892 0.6973971  0.8972617 ]], Bias: [-0.06453335]\n",
      "Epoch [15700/20001], Loss: 7.9317\n",
      "Weight: [[0.42509225 0.6965984  0.89823323]], Bias: [-0.06454303]\n",
      "Epoch [15800/20001], Loss: 7.9157\n",
      "Weight: [[0.42489663 0.69580376 0.89920044]], Bias: [-0.06455272]\n",
      "Epoch [15900/20001], Loss: 7.8997\n",
      "Weight: [[0.4247018 0.695012  0.9001633]], Bias: [-0.0645624]\n",
      "Epoch [16000/20001], Loss: 7.8840\n",
      "Weight: [[0.42450735 0.6942252  0.90112174]], Bias: [-0.06457209]\n",
      "Epoch [16100/20001], Loss: 7.8683\n",
      "Weight: [[0.42431363 0.6934421  0.9020756 ]], Bias: [-0.06458177]\n",
      "Epoch [16200/20001], Loss: 7.8528\n",
      "Weight: [[0.42412078 0.6926613  0.9030249 ]], Bias: [-0.06459146]\n",
      "Epoch [16300/20001], Loss: 7.8375\n",
      "Weight: [[0.4239279  0.6918856  0.90397125]], Bias: [-0.06460115]\n",
      "Epoch [16400/20001], Loss: 7.8222\n",
      "Weight: [[0.42373717 0.6911116  0.904913  ]], Bias: [-0.06461083]\n",
      "Epoch [16500/20001], Loss: 7.8072\n",
      "Weight: [[0.4235464 0.6903422 0.9058503]], Bias: [-0.06462052]\n",
      "Epoch [16600/20001], Loss: 7.7922\n",
      "Weight: [[0.42335567 0.6895758  0.906785  ]], Bias: [-0.0646302]\n",
      "Epoch [16700/20001], Loss: 7.7774\n",
      "Weight: [[0.4231656  0.68881285 0.90771484]], Bias: [-0.06463989]\n",
      "Epoch [16800/20001], Loss: 7.7626\n",
      "Weight: [[0.42297602 0.6880541  0.9086411 ]], Bias: [-0.06464957]\n",
      "Epoch [16900/20001], Loss: 7.7480\n",
      "Weight: [[0.42278734 0.6872971  0.9095641 ]], Bias: [-0.06465926]\n",
      "Epoch [17000/20001], Loss: 7.7336\n",
      "Weight: [[0.42259926 0.6865461  0.910482  ]], Bias: [-0.06466912]\n",
      "Epoch [17100/20001], Loss: 7.7193\n",
      "Weight: [[0.42241135 0.68579745 0.91139543]], Bias: [-0.06467955]\n",
      "Epoch [17200/20001], Loss: 7.7051\n",
      "Weight: [[0.4222236  0.6850524  0.91230595]], Bias: [-0.06468998]\n",
      "Epoch [17300/20001], Loss: 7.6910\n",
      "Weight: [[0.42203656 0.6843117  0.9132122 ]], Bias: [-0.06470041]\n",
      "Epoch [17400/20001], Loss: 7.6771\n",
      "Weight: [[0.42185047 0.68357295 0.91411406]], Bias: [-0.06471084]\n",
      "Epoch [17500/20001], Loss: 7.6633\n",
      "Weight: [[0.42166469 0.6828392  0.9150126 ]], Bias: [-0.06472127]\n",
      "Epoch [17600/20001], Loss: 7.6495\n",
      "Weight: [[0.4214799  0.682108   0.91590667]], Bias: [-0.0647317]\n",
      "Epoch [17700/20001], Loss: 7.6360\n",
      "Weight: [[0.4212952 0.6813804 0.9167966]], Bias: [-0.06474213]\n",
      "Epoch [17800/20001], Loss: 7.6225\n",
      "Weight: [[0.42111203 0.6806552  0.9176845 ]], Bias: [-0.06475256]\n",
      "Epoch [17900/20001], Loss: 7.6092\n",
      "Weight: [[0.4209294  0.67993397 0.91856664]], Bias: [-0.06476299]\n",
      "Epoch [18000/20001], Loss: 7.5959\n",
      "Weight: [[0.42074752 0.679215   0.91944635]], Bias: [-0.06477343]\n",
      "Epoch [18100/20001], Loss: 7.5828\n",
      "Weight: [[0.42056572 0.67849976 0.9203225 ]], Bias: [-0.06478386]\n",
      "Epoch [18200/20001], Loss: 7.5698\n",
      "Weight: [[0.42038393 0.6777886  0.9211936 ]], Bias: [-0.06479429]\n",
      "Epoch [18300/20001], Loss: 7.5569\n",
      "Weight: [[0.42020214 0.6770793  0.922063  ]], Bias: [-0.06480472]\n",
      "Epoch [18400/20001], Loss: 7.5441\n",
      "Weight: [[0.42002058 0.6763759  0.9229273 ]], Bias: [-0.06481515]\n",
      "Epoch [18500/20001], Loss: 7.5315\n",
      "Weight: [[0.41984063 0.6756742  0.92378813]], Bias: [-0.06482558]\n",
      "Epoch [18600/20001], Loss: 7.5189\n",
      "Weight: [[0.4196602  0.6749767  0.92464536]], Bias: [-0.06483601]\n",
      "Epoch [18700/20001], Loss: 7.5065\n",
      "Weight: [[0.4194814  0.67428243 0.9254979 ]], Bias: [-0.06484644]\n",
      "Epoch [18800/20001], Loss: 7.4942\n",
      "Weight: [[0.4193027  0.67359096 0.9263463 ]], Bias: [-0.06485687]\n",
      "Epoch [18900/20001], Loss: 7.4819\n",
      "Weight: [[0.4191254  0.67290276 0.9271927 ]], Bias: [-0.0648673]\n",
      "Epoch [19000/20001], Loss: 7.4698\n",
      "Weight: [[0.4189484  0.6722173  0.92803353]], Bias: [-0.06487773]\n",
      "Epoch [19100/20001], Loss: 7.4578\n",
      "Weight: [[0.41877237 0.67153454 0.9288727 ]], Bias: [-0.06488816]\n",
      "Epoch [19200/20001], Loss: 7.4459\n",
      "Weight: [[0.41859654 0.67085505 0.92970717]], Bias: [-0.0648986]\n",
      "Epoch [19300/20001], Loss: 7.4341\n",
      "Weight: [[0.4184207  0.67017895 0.930539  ]], Bias: [-0.06490903]\n",
      "Epoch [19400/20001], Loss: 7.4224\n",
      "Weight: [[0.4182451 0.6695054 0.9313675]], Bias: [-0.06491946]\n",
      "Epoch [19500/20001], Loss: 7.4108\n",
      "Weight: [[0.41806993 0.6688364  0.932192  ]], Bias: [-0.06492989]\n",
      "Epoch [19600/20001], Loss: 7.3993\n",
      "Weight: [[0.4178955  0.66816884 0.93301415]], Bias: [-0.06494032]\n",
      "Epoch [19700/20001], Loss: 7.3879\n",
      "Weight: [[0.41772193 0.6675064  0.933831  ]], Bias: [-0.06495075]\n",
      "Epoch [19800/20001], Loss: 7.3766\n",
      "Weight: [[0.41754848 0.6668451  0.9346447 ]], Bias: [-0.06496118]\n",
      "Epoch [19900/20001], Loss: 7.3654\n",
      "Weight: [[0.41737562 0.66618943 0.93545514]], Bias: [-0.06497165]\n",
      "Epoch [20000/20001], Loss: 7.3544\n",
      "Weight: [[0.41720384 0.66553444 0.9362608 ]], Bias: [-0.06498209]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20001\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    # 예측\n",
    "    y = model(x_data1)\n",
    "    \n",
    "    # 손실 계산\n",
    "    cost = criterion(y, t_data1)\n",
    "    \n",
    "    # 역전파\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {cost.item():.4f}')\n",
    "        print(f'Weight: {model.linear.weight.data.numpy()}, Bias: {model.linear.bias.data.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[172.0592],\n",
       "        [164.2733]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.FloatTensor([[75,85,90],[90,85,75]])\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
